# Redis

Redis cluster specification
https://redis.io/docs/reference/cluster-spec/

Redis常见面试题连环问，你能回答到第几问？（下） - xxx
https://juejin.cn/post/6844904046625554440

## 文件事件处理

Redis的文件事件与时间事件处理
https://blog.51cto.com/u_15696592/5422296

多个套接字、IO多路复用程序、文件事件分派器、事件处理器。
因为*文件事件分派器队列的消费是单线程的*，所以Redis才叫单线程模型

## 数据类型

五大基本类型分别是:

String, 这个是最常用的, 我用过存token,
Hash, 自增器(阅读量,点赞数. 定时写入数据库)
List : 时间轴. 比如按更新时间排序, 更新时间我们没上索引, 就是直接在Redis中存储了最近的1000条数据.
Set: 抽奖.
zSet : 排行榜
三大进阶类型:HyperLogLog, Geo、BloomFilter


Hash *ziplist => hashtable*
哈希对象的编码可以是 ziplist 或者 hashtable 。
http://redisbook.com/preview/object/hash.html

ziplist 编码的哈希对象使用*压缩列表*作为底层实现， 每当有新的键值对要加入到哈希对象时， 程序会先将保存了键的压缩列表节点推入到压缩列表表尾， 然后再将保存了值的压缩列表节点推入到压缩列表表尾， 因此：

1 保存了*同一键值对的两个节点总是紧挨在一起*， 保存键的节点在前， 保存值的节点在后；
2 先添加到哈希对象中的键值对会被放在压缩列表的表头方向， 而后来添加到哈希对象中的键值对会被放在压缩列表的表尾方向。

当哈希对象可以同时满足以下两个条件时， 哈希对象使用 ziplist 编码：
1 哈希对象保存的所有键值对的键和值的*字符串长度*都小于 64 字节；
2 哈希对象保存的键值对*数量*小于 512 个；
不能满足这两个条件的哈希对象需要使用 hashtable 编码。

*keys指令*会导致线程*阻塞*一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。
这个时候可以使用scan指令，*scan指令*可以*无阻塞*的提取出指定模式的key列表，但是会有一定的*重复*概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。

## 客户端

Redis 客户端 Jedis、lettuce 和 Redisson 对比 
https://www.cnblogs.com/54chensongxia/p/13815761.html

1. Jedis

Jedis 是老牌的 Redis 的 Java 实现客户端，提供了比较全面的 Redis 命令的支持，其官方网址是：http://tool.oschina.net/uploads/apidocs/redis/clients/jedis/Jedis.html。

优点：
支持全面的 Redis 操作特性（可以理解为API比较全面）。

缺点：
使用阻塞的 I/O，且其方法调用都是*同步的*，程序流需要等到 sockets 处理完 I/O 才能执行，不支持异步；
Jedis 客户端实例*不是线程安全的*，所以需要通过连接池来使用 Jedis。

2. lettuce
lettuce （[ˈletɪs]），是一种可扩展的*线程安全的* Redis 客户端，支持*异步模式*。如果避免阻塞和事务操作，如BLPOP和MULTI/EXEC，多个线程就可以共享一个连接。lettuce 底层基于 Netty，*支持高级的 Redis 特性*，比如哨兵，集群，管道，自动重新连接和Redis数据模型。lettuce 的官网地址是：https://lettuce.io/

优点：
支持同步异步通信模式；
Lettuce 的 API 是线程安全的，如果不是执行阻塞和事务操作，如BLPOP和MULTI/EXEC，多个线程就可以共享一个连接。

3. Redisson
Redisson 是一个在 Redis 的基础上实现的 Java 驻内存数据网格（In-Memory Data Grid）。它不仅提供了一系列的*分布式的 Java 常用对象*，还提供了许多*分布式服务*。其中包括( BitSet, Set, Multimap, SortedSet, Map, List, Queue, BlockingQueue, Deque, BlockingDeque, Semaphore, Lock, AtomicLong, CountDownLatch, Publish / Subscribe, Bloom filter, Remote service, Spring cache, Executor service, Live Object service, Scheduler service) Redisson 提供了使用Redis 的最简单和最便捷的方法。
Redisson 的宗旨是促进使用者对Redis的关注分离（Separation of Concern），从而让使用者能够将精力更集中地放在处理业务逻辑上。Redisson的官方网址是：https://redisson.org/

优点：
使用者对 Redis 的关注分离，可以类比 Spring 框架，这些框架搭建了应用程序的基础框架和功能，提升开发效率，让开发者有更多的时间来关注业务逻辑；
提供很多分布式相关操作服务，例如，分布式锁，分布式集合，可通过Redis支持延迟队列等。

缺点：
Redisson 对字符串的操作支持比较差。

## 持久化

Redis 提供两种持久化机制 RDB（默认） 和 AOF 机制:

RDB
按照一定的时间, fork 子进程来将内存的数据以*快照*的形式保存到硬盘中, dump.rdb

AOF(Append Only File)
则是将Redis执行的每次*写命令*记录到单独的*日志文件*中，当重启Redis会重新将持久化的日志中文件恢复数据
AOF 文件比 RDB 文件大，且恢复速度慢。

## 过期策略

**定时过期**：每个设置过期时间的key都需要创建一个*定时器*，到过期时间就会立即清除。
该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的CPU资源去处理过期的数据，从而影响缓存的响应时间和吞吐量。
**惰性过期**：只有*当访问一个key时*，才会判断该key是否已过期，过期则清除。
该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存。
**定期过期**：*每隔一定的时间*，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。
该策略是前两者的一个折中方案。
通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果。 (expires字典会保存所有设置了过期时间的key的过期时间数据，其中，key是指向键空间中的某个键的指针，value是该键的
毫秒精度的UNIX时间戳表示的过期时间。键空间是指该Redis集群中保存的所有键。)

## 内存淘汰策略

noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。
allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。（这个是最常用的）
allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。设置过期时间的键空间选择性移除
volatile-lru：当内存不足以容纳新写入数据时，在*设置了过期时间的键空间*中，移除最近最少使用的key。
volatile-random：当内存不足以容纳新写入数据时，在*设置了过期时间的键空间*中，随机移除某个key。
volatile-ttl：当内存不足以容纳新写入数据时，在*设置了过期时间的键空间*中，有更早过期时间的key优先移除。

LRU和LFU 算法（页面置换算法）
https://blog.csdn.net/weixin_43240734/article/details/123159387
LRU和LFU都是内存管理的页面置换算法。
LRU：最近最少使用(最长时间)淘汰算法（Least Recently Used）。LRU是淘汰最长时间没有被使用的页面。
LFU：最不经常使用(最少次)淘汰算法（Least Frequently Used）。LFU是淘汰一段时间内，使用次数最少的页面。

## Redis事务

Redis事务功能是通过MULTI、EXEC、DISCARD和WATCH 四个原语实现的
Redis会将一个事务中的所有命令序列化，然后按顺序执行。
1. redis *不支持回滚*，“Redis 在事务失败时不进行回滚，而是继续执行余下的命令”， 所以 Redis 的内部可以保持简单且快速。
2. 如果在一个事务中的命令出现错误，那么所有的命令都不会执行；- 语法错误, 事务提交失败
3. 如果在一个事务中出现运行错误，那么正确的命令会被执行。- 类型错误, 运行时失败

*WATCH 命令*是一个乐观锁，可以为 Redis 事务提供 check-and-set （CAS）行为。 
可以监控一个或多个键，*一旦其中有一个键被修改（或删除），之后的事务就不会执行*，监控一直持续到EXEC命令。
*UNWATCH命令*可以取消watch对所有key的监控。

*MULTI命令*用于*开启一个事务*，它总是返回OK。 
MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行。

*EXEC*：*执行*所有事务块内的命令。*返回事务块内所有命令的返回值*，按命令执行的先后顺序排列。 当操作被打断时，返回空值 nil 。
通过调用*DISCARD*，客户端可以清空事务队列，并放弃执行事务， 并且客户端会从事务状态中退出。

基于*Lua脚本*，Redis可以保证脚本内的命令一次性、按顺序地执行，
其同时也*不提供事务运行错误的回滚*，执行过程中如果部分命令运行错误，剩下的命令还是会继续运行完

## Redis分布式有哪些架构? 分别什么特点?

Redis **主从架构**, 一般都是*读写分离*的, 可靠性不足, 无法自动完成主从切换.
redis 采用*异步*方式复制数据到 slave 节点，slave node 会周期性地确认自己每次复制的数据量
1.当启动一个 slave node 的时候，它会发送一个 `PSYNC` 命令给 master node。
2.如果这是 slave node 初次连接到 master node，那么会触发一次 full resynchronization *全量复制*。此时 master 会启动一个后台线程，开始生成一份 *RDB* 快照文件，
3.同时还会将从客户端 client 新收到的所有*写命令缓存在内存中*。 RDB 文件生成完毕后， master会将这个 RDB 发送给 slave，slave 会先写入本地磁盘，然后再从本地磁盘加载到内存中，
4.接着 master 会*将内存中缓存的写命令发送到 slave*，slave 也会同步这些数据。
5.slave node 如果跟 master node 有网络故障，断开了连接，会*自动重连*，连接之后 master node仅会复制给 slave 部分缺少的数据

Redis Sentinal **哨兵架构**, 可以监控Redis节点, 能够故障自动迁移.
哨兵至少需要 3 个实例，来保证自己的健壮性。
故障转移时，判断一个 master node 是否宕机了，需要*大部分的哨兵*都同意才行，涉及到了分布式选举的问题。
哨兵 + redis 主从的部署架构，是*不保证数据零丢失的*，只能保证 redis 集群的*高可用性*。

Redis Cluster **集群架构**, 可以去中心化, 能够实现故障自动迁移, 数据分片. 并且优化了哨兵迁移时的连接中断问题.
一种服务端Sharding技术, 采用slot(槽)的概念，一共分成*16384个槽*
每个key通过*CRC16校验*后对16384取模来决定放置哪个槽，集群的每个节点负责一部分hash槽
将请求发送到任意节点，接收到请求的节点会将查询请求*发送到正确的节点上*执行

1. 通过哈希的方式，将数据分片，每个节点均分存储一定哈希槽(哈希值)区间的数据，默认分配了16384 个槽位
2. 每份数据分片会存储在多个*互为主从*的多节点上
3. 数据写入*先写主节点*，再*同步到从节点*(支持配置为阻塞同步)
4. 同一分片多个节点间的数据不保持一致性
5. 读取数据时，当客户端操作的key没有分配在该节点上时，redis会返回*转向指令*，指向正确的节点
6. 扩容时时需要需要把旧节点的数据*迁移*一部分到新节点

## Redis 分布式锁了解吗? 如何提升分布式锁的性能? 如何保证分布式下分布式锁失效的问题 ?

Redis 分布式锁主要时为了解决多实例应用下, 实例之间的并发不安全问题, 比如库存超卖问题.
通过将锁放在Redis中, 保证实例, 线程之间是并发安全的. 实现分布式锁的命令是setnx, 在Java中我们往往不会选用原生的Redis去实现分布式锁, 因为需要考虑的事情太多, 比如超时问题,加锁与解锁的原子性问题.

在开发中,我们使用**Redisson**来解决分布式锁, 它自带**超时补偿机制**解决超时问题, **使用Lua脚本解决锁操作的原子性问题**.
分布式锁的性能在于把原来并行的线程编程了串行化的命令, 这就带来了性能问题, 这种情况一般分两种思路并行处理: 1. 降低锁的粒度, 减少需要锁的时间; 2. 采用CHM的分段锁, 比如库存拆分成多份库存.

Redisson能保证代码级别的正确性, 但是因为*Redis集群满足的是AP*, 那么就会存在宕机后的不一致问题, 这种情况, 可以考虑采用ZK集群来替换Redis集群, 或者使用RedLock降低性能, 手动给半数以上的Redis发送setnx命令.

## 缓存雪崩,缓存击穿,缓存穿透

- 缓存雪崩

> 大量缓存数据同时过期, redis故障

**均匀设置过期时间**, 在原来的过期时间上加一些随机数.
**互斥锁**, 就是让第一个没有拿到缓存的数据设置一个超时锁, 然后再去数据库更新缓存, 这样之后的请求就会阻塞在超时锁上, 而不是打崩数据库.
**双 key 策略**, 就是设置一个超时变量, 一个不超时的变量, 更新的时候同时维护这两份. 相当于熔断操作.
**后台更新缓存**, 长期有效, 交由后台更新缓存数据, 此外为了避免缓存被LRU淘汰后失效 ,还需要定期刷缓存, 或者业务线程发现后通过MQ通知刷新缓存. 后者效果更好一些.

Redis 宕机的处理:

搭建高可用集群, 主要是为了故障切换, 减少宕机带来的影响
熔断或者限流访问数据库.

- 缓存击穿

> 热点数据缓存过期

​ 缓存击穿是缓存雪崩的一个子集, 解决方案主要是互斥锁 or 双key or长期有效,后台更新

- 缓存穿透

> 数据即不在缓存, 也不在数据库

*布隆过滤器*来处理, 此处不再展开.
非法请求的限制.
缓存*空值*或者默认值.

## Redis大Key解决方案

1. 单个K存储的V很大

key分类如下：

该key需要每次都*整存整取*
尝试将对象分拆成几个K.V， 使用*multiGet*获取值。
 拆分旨在降低单次操作的压力，将操作压力平摊到多个Redis实例，降低对单个redis的I/O影响。

该对象每次只需要*存取部分数据*
类似上一种方案，拆分成几个K.V
也可将这个大对象存储在一个*hash*，每个field代表一个具体属性

2. 一个集群存储了上亿key

key本身具备*强相关性*
比如多个K代表一个对象，每个K是对象的一个属性，这种可直接按照特定对象的特征来设置一个新K——*Hash*结构， 原先的K则作为这个新Hash 的field。

key本身*无相关性*
预估总量，预分一个固定的*桶*数量：
 比如现在预估K总计2亿，按一个hash存储 100个field算，需要 2亿 / 100 = 200W 个桶 (200W 个K占用的空间很少，2亿可能有近20G )。
 现在按200W固定桶分，即先计算出桶的序号
原先比如有三个key ： user.123456789 , user.987654321， user.678912345
现在按照200W 固定桶分就是先计算出桶的序号 hash(123456789) % 200W ， 这里最好保证这个 hash算法的值是个正数，否则需要调整下模除的规则；
这样算出三个key 的桶分别是 1 ， 2， 2。 
所以存储的时候调用API hset(key, field, value)，读取的时候使用 hget(key， field)
key1 : hset （ **user.1**, **123456789** , value ） hget（ user.1, 123456789）
key2: hset ( user.2, 987654321, value ) hget（user.2, 987654321）
key3: hset （ user.2, 678912345, value) hget（user.2, 678912345）
注意两个地方：1.hash 取模对负数的处理； 2.预分桶的时候， 一个hash 中存储的值最好不要超过 512 ，100 左右较为合适

发现并处理Redis的大Key和热Key
https://help.aliyun.com/document_detail/353223.html

> 删除大key

4.0 以前 string，list，set，hash 不同数据类型的大 key，删除方式有所不同。一般有两种情况：
**del**命令**删除单个**很大的 key 和 
del **批量删除** 大 key。
直接 del 命令粗暴的删大 key 容易造成 redis **线程阻塞**。
4.0 以前要优雅的删除就是针对不同的类型 写脚本，拆分链表，hash 表，分批删除。

Redis 4.0 以后优雅的删除大 key

**主动删除**
**UNLINK** xxxkey
unlink 命令是 del 的**异步**版本，由 **Lazyfree** 机制实现。
Lazyfree 机制的原理是在删除的时候只进行**逻辑删除**，把 key **释放操作**放在 bio (Background I/O)**单独的子线程**中惰性处理，减少删除大 key 对 redis 主线程的阻塞，有效地避免因删除大key带来的性能问题。
unlink 即使在批量删除 大 key 时，也不会对阻塞造成阻塞。

**被动删除**
被动删除是指 Redis 自身的 key **清除策略**，一个 大 key 过期或者被淘汰时，如何被清除，会不会导致阻塞？
4.0 以前自动清除是有可能阻塞主线程的。
4.0 以后的版本，被动删除策略是可选的配置参数，允许以 **Lazyfree** 的方式清除。但是参数默认是关闭的，可配置如下参数开启。
lazyfree-lazy-expire on # 过期惰性删除
lazyfree-lazy-eviction on # 超过最大内存惰性删除
lazyfree-lazy-server-del on # 服务端被动惰性删除

## 如何保证缓存与数据库双写时的数据一致性

https://blog.csdn.net/qq_44373419/article/details/137024994

读请求和写请求串行化

- 延时双删

1. 先删除缓存
2. 再更新数据库
3. 休眠一会（比如 1 秒），再次删除缓存

休眠那一会（比如就那 1 秒），可能有脏数据，一般业务也会接受。但如果第二次删除缓存失败呢？
给 Key 设置一个自然的expire 过期时间，让它自动过期怎样？

- 删除缓存重试流程

1. 写请求更新数据库
2. 缓存因为某些原因，删除失败
3. 把删除失败的key 放到消息队列
4. 消费消息队列✁消息，获取要删除的key
5. 重试删除缓存操作

- 读取binlog 异步删除缓存

可以使用阿里canal 将 binlog 日志采集发送到 MQ 队列里面
然后通过 ACK 机制确认处理这条更新消息，删除缓存，保证数据缓存一致性

## Redis集群-哨兵模式原理（Sentinel）

https://blog.csdn.net/BruceLiu_code/article/details/125904703

redis集群Sentinel实现原理
https://blog.csdn.net/m0_54434140/article/details/122492743

1.哨兵模式（Sentinel）
哨兵模式是Redis的高可用性解决方案：由*一个或多个Sentinel实例组成的Sentinel系统*可以同时*监控*r
在监视到主服务器进入下线状态时，自动将下线主服务器属下的某个从服务器*升级*为新的主服务器，然后由新的主服务器代替已下线的主服务器继续处理命令请求。

示意图


双环代表主服务器server1
单环代表三个从服务器server2、server3、server4
server2、server3、server4三个从服务器正在复制主服务器server1，而sentinel系统正在监听所有四个服务器
可以看出哨兵模式就是在主从复制模式之上添加了哨兵系统，从而实现故障的自动转移

故障转移


如上图16-3所示，主服务server1挂掉了，处于下线状态，那么server2、server3、server4对主服务器的复制操作将被终止，并且隔一段时间sentinel系统也会察觉到server1的下线，下面先说一下故障转移的流程：

Sentinel系统会*挑选*server1属下的其中一个从服务器，并选中的从服务器升级为新的主服务器
Sentinel系统会向server1属下的所有从服务器*发送新的复制命令*，让他们成为新的主服务器的从服务器，当所有从服务器都开始复制新的主服务器时，故障转移操作执行完毕
Sentinel系统还会继续*监听已下线的server1*，如果它重新上线时，会将它设置为新的主服务器的从服务器

2.Sentinel系统与各个节点的通讯
2.2.1.Sentinel如何判断主服务器下线
*主观下线*

在默认情况下，Sentinel系统会以每秒一次的频率向所有与它创建了命令连接的实例发送PING命令，并通过实例返回的结果来判断实例是否在线。

如果一个实例在down-after-milliseconds毫秒内（默认30s），连续向Sentinel返回无效回复，那该Sentinel就会将其标记为主观下线状态。
Sentinal配置文件中的down-after-milliseconds选项指定了Sentinel判断实例进入主观下线所需的时间长度

*客观下线*
当Sentinel将一个主服务器判断为主观下线之后，为了确认这个主服务器是否真的下线了，它会向同样监视这一主服务器的其他Sentinel进行*询问*，看它们是否也认为主服务器已经是下线状态（可以是主观下线或客观下线），当Sentinel从其他Sentinel那里接受到*足够数量的*已下线判断之后，Sentinel就会将从服务器判定为客观下线，并对主服务器开始执行故障转移操作。

客观下线状态的判断条件：当认为主服务器已经进入下线状态的Sentinel的数量，超过Sentinel配置中设置的quorum参数的值，那么该Sentinel就会认为主服务器已经进入客观下线状态。

上面配置的含义：包括当前Sentinel在内，只要总共有两个Sentinel认为主服务器已经下线，那么当前Sentinel就将主服务器判断为客观下线。

当Sentinel将一个主服务器判断为主观下线后，它会向同样监视该主服务器的其他Sentinel进行询问，看它们是否同意这个主服务器已经进入主观下线状态。

3 .Sentinel如何判断主服务器下线

3.1.选举领头Sentinel
当一个主服务器被判断为客观下线时，*监视这个下线主服务器的各个Sentinel*会进行协商，*选举出一个领头Sentinel*，并*由领头Sentinel对下线主服务器执行故障转移操作*。

选举领头Sentinel的规则：

1.所有在线的Sentinel都有被选为领头Sentinel的资格，换句话说，监视同一个主服务器的 多个在线Sentinel中的任意一个都有可能成为领头Sentinel
2.每次进行领头Sentinel选举之后，不论选举是否成功，所有Sentinel的配置纪元 （configuration epoch）的值都会*自增*一次。 配置纪元实际上就是一个计数器，并没有什么特 别的。
3.在一个配置纪元里面， 所有Sentinel都有一次将某个Sentinel设置为*局部领头Sentinel*的机会 并且局部领头一旦设置，在这个配置每个发现主服务器进入客观下线的Sentinel都会要求其他Sentinel将自己设置为局部领头Sentinel。 - 投给自己
4.当一个Sentinel（源Sentinel）向另一个Sentinel（目标Sentinel）发送SENTINEL ismaster-down-by-addr命令，并且命令中的runid参数不是*符号而是源Sentinel的运行ID时，这表示源Sentinel要求目标Sentinel将前者设置为后者的局部领头Sentinel
5.Sentinel设置局部领头Sentinel的规则是*先到先得*：最先向目标Sentinel发送设置要求的源Sentinel将成为目标Sentinel的局部领头Sentinel，而之后接收到的所有设置要求都会被目标Sentinel拒绝
6.目标Sentinel在接收到SENTINEL is-master-down-by-addr命令之后，将向源Sentinel返回 一条命令*回复*，回复中的leader_runid参数和leader_epoch参数分别记录了目标Sentinel的局部领头Sentinel的运行ID和配置纪元
7.源Sentinel在接收到目标Sentinel返回的命令回复之后，会检查回复中leader_epoch参数 的值和自己的配置纪元是否相同，如果相同的话，那么源Sentinel继续取出回复中的 leader_runid参数， 如果leader_runid参数的值和源Sentinel的运行ID一致，那么表示目标 Sentinel将源Sentinel设置成了局部领头Sentinel
8.如果有某个Sentinel被半数以上的Sentinel设置成了局部领头Sentinel，那么这个Sentinel成 为领头Sentinel。举个例子，在一个由10个Sentinel组成的Sentinel系统里面，只要有大于等于 10/2+1=6个Sentinel将某个Sentinel设置为局部领头Sentinel，那么被设置的那个Sentinel就会成 为领头Sentine
9.因为领头Sentinel的产生需要半数以上Sentinel的支持，并且每个Sentinel在每个配置纪元里面只能设置一次局部领头Sentinel，所以在一个配置纪元里面，只会出现一个领头 Sentinel
10.如果在给定时限内，没有一个Sentinel被选举为领头Sentinel，那么各个Sentinel将在一段时间之后再次进行选举，直到选出领头Sentinel为止

3.2.新主服务器的选举&故障转移

选举好领头Sentinel之后，领头Sentinel将对已下线的服务器执行故障转移操作。
第一步：首先会从已下线主服务器（server1）属下所有的从服务器中*挑选*一个状态良好、数据完整的从服务器，并发送SLAVE no one命令。
第二步：此时领头Sentinel会以每秒一次的频率（平时十秒一次）向被升级的从服务器（server2）发送*INFO命令*并观察返回的*role*是否已经变成了master，变成master说明升级成功。
第三步：领头Sentinel向已下线主服务器（server1）的两个从服务器（server3、server4）发送*SLAVEOF命令*，让他们复制新的主服务器（server2）。
第四步：当server1*重新上线*时，Sentinel就会向它发送*SLAVEOF命令*，让它成为新主服务器（server2）的从服务器。

至此，整个故障转移就完成了。

## Redis——cluster集群原理

https://blog.csdn.net/weixin_41605937/article/details/114779041

摘要
在 redis3.0之前，redis使用的哨兵架构，它借助 *sentinel* 工具来监控 master 节点的状态；如果 master 节点异常，则会做*主从切换*，将一台 slave 作为 master。当master挂掉的时候，sentinel 会选举出来一个 master，*选举的时候是没有办法去访问Redis的*，会存在访问瞬断的情况；若是在电商网站大促的时候master给挂掉了，几秒钟损失好多订单数据；哨兵模式，*对外只有master节点可以写*，slave节点只能用于读。尽管Redis单节点最多支持10W的QPS，但是在电商大促的时候，写数据的压力全部在master上。Redis的单节点内存不能设置过大，若数据过大在主从同步将会很慢；在节点启动的时候，时间特别长；（从节点上有主节点的所有数据）。

一、redis集群架构与数据存储原理
Redis集群是一个由*多个主从节点群*组成的分布式服务集群，它具有*复制*、*高可用*和*分片*特性。Redis集群不需要sentinel哨兵也能完成节点移除和故障转移的功能。需要将每个节点设置成集群模式，这种集群模式*没有中心节点*，可水平扩展，据官方文档称可以线性扩展到上万个节点(官方推荐不超过1000个节点)。
redis集群的性能和高可用性均优于之前版本的哨兵模式，且集群配置非常简单。
Redis 集群是一种分布式数据库方案，集群通过*分片（sharding）*来进行数据管理（分治思想的一种实践），并提供*复制*和*故障转移功能*。将数据划分为 16384 的 slots，每个节点负责一部分槽位。槽位的信息存储于每个节点中。它是去中心化的，如图所示，该集群有三个 Redis 节点组成，每个节点负责整个集群的一部分数据，每个节点负责的数据多少可能不一样。三个节点相互连接组成一个对等的集群，它们之间通过 Gossip协议相互交互集群信息，最后每个节点都保存着其他节点的 slots 分配情况。​​​​​​​

Redis集群的优点：

Redis集群有*多个master*，可以减小访问瞬断问题的影响；若集群中有一个master挂了，正好需要向这个master写数据，这个操作需要等待；但向其他master节点写数据不受影响。
Redis集群有多个master，可以提供更高的并发量；
Redis集群可以*分片存储*，这样就可以存储更多的数据；
使用 Redis Cluster 集群，主要解决了大数据量存储导致的各种慢问题，同时也便于横向拓展

两种方案对应着 Redis 数据增多的两种拓展方案：垂直扩展（scale up）、水平扩展（scale out）。

垂直拓展：升级单个 Redis 的硬件配置，比如增加内存容量、磁盘容量、使用更强大的 CPU。
水平拓展：横向增加 Redis 实例个数，每个节点负责一部分数据。
比如需要一个内存 24 GB 磁盘 150 GB 的服务器资源，有以下两种方案：



在面向百万、千万级别的用户规模时，横向扩展的 Redis 切片集群会是一个非常好的选择。

垂直拓展部署简单，但是当数据量大并且使用 RDB 实现持久化，会造成阻塞导致响应慢。另外受限于硬件和成本，拓展内存的成本太大，比如拓展到 1T 内存。
水平拓展便于拓展，同时不需要担心单个实例的硬件和成本的限制。但是，切片集群会涉及多个实例的分布式管理问题，需要解决如何将数据合理分布到不同实例，同时还要让客户端能正确访问到实例上的数据。

Redis Cluster 具有以下特点：

节点互通：所有的 Redis 节点彼此互联（PING-PONG机制），内部使用二进制协议优化传输速度和带宽；
*去中心化*：Redis Cluster 不存在中心节点，每个节点都记录有集群的状态信息，并且通过 Gossip 协议，使每个节点记录的信息实现最终一致性；
*客户端直连*：客户端与 Redis 节点直连，不需要中间 Proxy 层，客户端不需要连接集群所有节点，连接集群中任何一个可用节点即可；
*数据分片*：Redis Cluster 的键空间被分割为 16384 个 Slot，这些 Slot 被分别指派给主节点，当存储 Key-Value 时，根据 CRC16(key) Mod 16384的值，决定将一个 Key-Value 放到哪个 Slot 中；
*多数派原则*：对于集群中的任何一个节点，需要超过半数的节点检测到它失效（pFail），才会将其判定为失效（Fail）；
*自动 Failover*：当集群中某个主节点故障后（Fail），其它主节点会从故障主节点的从节点中选举一个“最佳”从节点升主，替代故障的主节点；
功能弱化：集群模式下，由于数据分布在多个节点，不支持单机模式下的集合操作，也不支持多数据库功能，集群*只能使用默认的0号数据库*；
集群规模：官方推荐的最大节点数量为 1000 个左右，这是因为当集群规模过大时，Gossip 协议的效率会显著下降，通信成本剧增。

1.1 redis数据分片原理
集群的整个数据库被分为 16384 个槽（slot），数据库中的每个键都属于这 16384 个槽的其中一个，集群中的每个节点可以处理 0 个或最多 16384 个槽。

Key 与哈希槽映射过程可以分为两大步骤：

根据键值对的 key，使用 CRC16 算法，计算出一个 16 bit 的值；
将 16 bit 的值对 16384 执行取模，得到 0 ～ 16383 的数表示 key 对应的哈希槽。
Cluster 还允许用户强制某个 key 挂在特定槽位上，通过在 key 字符串里面嵌入 *tag* 标记，这就可以强制 key 所挂在的槽位等于 tag 所在的槽位。

1.2 Redis Cluster 请求路由方式
客户端*直连* Redis 服务，进行读写操作时，Key 对应的 Slot 可能并不在当前直连的节点上，经过“重定向”才能转发到正确的节点。

和普通的查询路由相比，Redis Cluster 借助客户端实现的请求路由是一种混合形式的查询路由，它并非从一个 Redis 节点到另外一个 Redis，而是借助客户端转发到正确的节点。实际应用中，可以在*客户端缓存 Slot 与 Redis 节点的映射关系*，当接收到 *MOVED* 响应时*修改*缓存中的映射关系。如此，基于保存的映射关系，请求时会直接发送到正确的节点上，从而减少一次交互，提升效率。

客户端又怎么确定访问的数据到底分布在哪个实例上呢？

Redis 实例会将自己的哈希槽信息通过 *Gossip* 协议发送给集群中其他的实例，实现了哈希槽分配信息的扩散。这样，集群中的每个实例都有所有哈希槽与实例之间的映射关系信息。

在切片数据的时候是将 key 通过 *CRC16* 计算出一个值再对 16384 取模得到对应的 Slot，这个计算任务可以在客户端上执行发送请求的时候执行。但是，定位到槽以后还需要进一步定位到该 Slot 所在 Redis 实例。
当客户端连接任何一个实例，实例就将哈希槽与实例的映射关系响应给客户端，客户端就会将哈希槽与实例映射信息缓存在本地。
当客户端请求时，会计算出键所对应的哈希槽，在通过本地缓存的哈希槽实例映射信息定位到数据所在实例上，再将请求发送给对应的实例。

哈希槽与实例之间的映射关系由于新增实例或者负载均衡重新分配导致改变了咋办？

集群中的实例通过 Gossip 协议互相传递消息获取最新的哈希槽分配信息，但是，客户端无法感知。
Redis Cluster 提供了重定向机制：客户端将请求发送到实例上，这个实例没有相应的数据，该 Redis 实例会告诉客户端将请求发送到其他的实例上。

Redis 如何告知客户端重定向访问新实例呢？分为两种情况：MOVED 错误、ASK 错误。

*MOVED 错误*：
MOVED 错误（负载均衡，数据已经迁移到其他实例上）：当客户端将一个键值对操作请求发送给某个实例，而这个键所在的槽并非由自己负责的时候，该实例会返回一个 MOVED 错误指引*转向正在负责该槽的节点*。

(error) MOVED 16330 172.17.18.2:6379
该响应表示客户端请求的键值对所在的哈希槽 16330 迁移到了 172.17.18.2 这个实例上，端口是 6379。这样客户端就与 172.17.18.2:6379 建立连接，并发送 GET 请求。同时，客户端还会*更新本地缓存*，将该 slot 与 Redis 实例对应关系更新正确。

*ASK 错误*：
如果某个 slot 的数据比较多，*部分迁移*到新实例，还有一部分没有迁移咋办？
如果请求的 key 在当前节点找到就直接执行命令，否则时候就需要 ASK 错误响应了，槽部分迁移未完成的情况下，如果需要访问的 key 所在 Slot 正在从从 实例 1 迁移到 实例 2，实例 1 会返回客户端一条 ASK 报错信息：客户端请求的 key 所在的哈希槽正在迁移到实例 2 上，你先给实例 2 发送一个 ASKING 命令，接着发发送操作命令。

(error) ASK 16330 172.17.18.2:6379
比如客户端请求定位到 key的槽16330 在实例 172.17.18.1 上，节点1如果找得到就直接执行命令，否则响应 ASK 错误信息，并指引客户端转向正在迁移的目标节点 172.17.18.2。

注意：ASK 错误指令并*不会更新客户端缓存的哈希槽分配信息*。所以客户端再次请求 Slot 16330 的数据，还是会先给 172.17.18.1 实例发送请求，只不过节点会响应 ASK 命令让客户端给新实例发送一次请求。MOVED指令则更新客户端本地缓存，让后续指令都发往新实例。

1.3 Redis的一致性哈希算法
采用一致性哈希算法(consistent hashing)，将key和*节点name*同时hashing，然后进行映射匹配，采用的算法是MURMUR_HASH。采用一致性哈希而不是采用简单类似哈希求模映射的主要原因是当增加或减少节点时，不会产生由于重新匹配造成的rehashing。*一致性哈希只影响相邻节点key分配*，影响量小。

为了避免一致性哈希只影响相邻节点造成节点分配压力，ShardedJedis会对每个Redis节点根据名字(没有，Jedis会赋予缺省名字)会虚拟化出160个*虚拟节点*进行散列。根据权重weight，也可虚拟化出160倍数的虚拟节点。用虚拟节点做映射匹配，可以在增加或减少Redis节点时，key在各Redis节点移动再分配更均匀，而不是只有相邻节点受影响。

Hash环的*数据倾斜*问题：一致性Hash算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜（被缓存的对象大部分集中缓存在某一台服务器上）问题，例如系统中只有两台服务器，其环分布如图所示，此时必然造成大量数据集中到Node A上，而只有极少量会定位到Node B上。为了解决这种数据倾斜问题，一致性Hash算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为*虚拟节点*。具体做法可以在服务器IP或主机名的后面增加编号来实现。例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点：

二、redis集群选举算法
2.1 *集群初始选举算法*
集群的配置纪元 +1，是一个自曾计数器，初始值 0 ，每次执行故障转移都会 +1。
检测到主节点下线的从节点向集群广播一条CLUSTERMSG_TYPE_FAILOVER_AUTH_REQUEST消息，要求所有收到这条消息、并且具有投票权的主节点向这个从节点投票。
这个主节点尚未投票给其他从节点，那么主节点将向要求投票的从节点返回一条CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK消息，表示这个主节点支持从节点成为新的主节点。
参与选举的从节点都会接收CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK消息，如果收集到的票 >= (N/2) + 1 支持，那么这个从节点就被选举为新主节点。
如果在一个配置纪元里面没有从节点能收集到足够多的支持票，那么集群进入一个新的配置纪元，并再次进行选举，直到选出新的主节点为止。
跟哨兵类似，两者都是基于 *Raft 算法*来实现的，流程如图所示：


三、redis集群通信原理
Redis 自3.0版本起，支持 Redis Cluster，真正意义上实现了分布式。在分布式系统中，节点间的通信十分重要，是构建集群的基石。那么 Redis Cluster 中，节点间是如何通信的呢？又是如何保障一致性、可用性的呢？欲知答案，必先了解 *Gossip* 算法。Gossip 算法源自流行病学的研究，经过不断的发展演化，作为一种分布式一致性协议而得到广泛应用，如 Cassandra、Akka、Redis 都有用到。

Gossip 特点：在一个有界网络中，每个节点都随机地与其它节点通信，*经过一番杂乱无章的通信，最终所有节点的状态都会达成一致*。每个节点可能知道所有其它节点，也可能仅知道几个邻居节点，只要这些节可以通过网络连通，最终它们的状态都是一致的。要注意到的一点是，即使有的节点因宕机而重启，有新节点加入，但经过一段时间后，这些节点的状态也会与其他节点达成一致，也就是说，Gossip 天然具有分布式容错的优点。

Gossip 是一个*带冗余的容错算法*，更进一步，Gossip 是一个*最终一致性算法*。虽然无法保证在某个时刻所有节点状态一致，但可以保证在“最终”所有节点一致，“最终”是一个现实中存在，但理论上无法证明的时间点。因为 Gossip 不要求节点知道所有其它节点，因此又具有去中心化的特点，节点之间完全对等，不需要任何的中心节点。实际上 Gossip 可以用于众多能接受“最终一致性”的领域：失败检测、路由同步、Pub/Sub、动态负载均衡。但 Gossip 的缺点也很明显，冗余通信会对网路带宽、CUP 资源造成很大的负载，而这些负载又受限于通信频率，该频率又影响着算法收敛的速度，

Gossip 在 Redis Cluster 中的作用：

在分布式系统中，需要提供*维护节点元数据信息*的机制，所谓元数据是指节点负责哪些数据、主从属性、是否出现故障等状态信息。常见的元数据维护方式分为集中式和无中心式。Redis Cluster 采用 Gossip 协议实现了无中心式。Redis Cluster 中使用 Gossip 主要有两大作用：

去中心化，以实现分布式和弹性扩展；
失败检测，以实现高可用；

3.1 Gossip 消息种类
发送的消息结构是 clusterMsgDataGossip结构体组成：

typedef struct {
    char nodename[CLUSTER_NAMELEN];  //40字节
    uint32_t ping_sent; //4字节
    uint32_t pong_received; //4字节
    char ip[NET_IP_STR_LEN]; //46字节
    uint16_t port;  //2字节
    uint16_t cport;  //2字节
    uint16_t flags;  //2字节
    uint32_t notused1; //4字节
} clusterMsgDataGossip;
所以每个实例发送一个 Gossip消息，就需要发送 104 字节。如果集群是 1000 个实例，那么每个实例发送一个 PING 消息则会占用 大约 10KB。除此之外，实例间在传播 Slot 映射表的时候，每个消息还包含了 一个长度为 16384 bit 的 Bitmap。每一位对应一个 Slot，如果值 = 1 则表示这个 Slot 属于当前实例，这个 Bitmap 占用 2KB，所以一个 PING 消息大约 12KB。PONG与PING 消息一样，一发一回两个消息加起来就是 24 KB。集群规模的增加，心跳消息越来越多就会占据集群的网络通信带宽，降低了集群吞吐量。

Gossip 协议的主要职责就是信息交换。信息交换的载体就是节点彼此发送的Gossip 消息，常用的 Gossip 消息可分为：Ping 消息、Pong 消息、Meet 消息、Fail 消息。

*Meet* 消息：用于通知*新节点加入*。消息发送者通知接收者加入到当前集群，Meet 消息通信正常完成后，接收节点会加入到集群中并进行周期性的 Ping、Pong 消息交换；
*Ping* 消息：集群内交换最频繁的消息，集群内每个节点每秒向多个其它节点发送 Ping 消息，用于检测节点是否在线和交换彼此状态信息。Ping 消息发送封装了自身节点和部分其它节点的*状态数据*；
*Pong* 消息：当接收到 Ping、Meet 消息时，作为响应消息回复给发送方确认消息正常通信。Pong 消息内部封装了*自身状态数据*。节点也可以向集群内广播自身的 Pong 消息来*通知整个集群对自身状态进行更新*；
*Fail* 消息：当节点判定集群内*另一个节点下线*时，会向集群内广播一个 Fail 消息，其他节点接收到 Fail 消息之后把对应节点更新为下线状态。
由于集群内部需要频繁地进行节点信息交换，而 Ping/Pong 消息携带当前节点和部分其它节点的状态数据，势必会加重带宽和计算的负担。Redis 集群内节点通信采用固定频率（定时任务每秒执行10次），因此，节点每次选择需要通信的节点列表变得非常重要。通信节点选择过多虽然可以做到信息及时交换但成本过高。节点选择过少则会降低集群内所有节点彼此信息交换的频率，从而影响故障判定、新节点发现等需求的速度。因此 Redis 集群的 Gossip 协议需要兼顾信息交换实时性和成本开销。

发送 PING 消息的频率也会影响集群带宽吧？

Redis Cluster 的实例启动后，默认会每秒从本地的实例列表中随机选出 5 个实例，再从这 5 个实例中找出一个最久没有收到 PING 消息的实例，把 PING 消息发送给该实例。

随机选择 5 个，但是无法保证选中的是整个集群最久没有收到 PING 通信的实例，有的实例可能一直没有收到消息，导致他们维护的集群信息早就过期了，咋办呢？

这个问题问的好，Redis Cluster 的实例每 100 ms 就会扫描本地实例列表，当发现有实例最近一次收到 PONG 消息的时间 > cluster-node-timeout / 2。那么就立刻给这个实例发送 PING 消息，更新这个节点的集群状态信息。当集群规模变大，就会进一步导致实例间网络通信延迟怎加。可能会引起更多的 PING 消息频繁发送。

3.1.1 降低实例间的通信开销
每个实例每秒发送一条 PING消息，降低这个频率可能会导致集群每个实例的状态信息无法及时传播。
每 100 ms 检测实例 PONG消息接收是否超过 cluster-node-timeout / 2，这个是 Redis 实例默认的周期性检测任务频率，我们不会轻易修改。
所以，只能修改 cluster-node-timeout的值：集群中判断实例是否故障的心跳时间，默认 15 S。所以，为了避免过多的心跳消息占用集群宽带，将 cluster-node-timeout调成 20 秒或者 30 秒，这样 PONG 消息接收超时的情况就会缓解。但是，也不能设置的太大。都则就会导致实例发生故障了，等待 cluster-node-timeout时长才能检测出这个故障，影响集群正常服务、

五、redis集群的故障转移原理

如果某个主节点没有从节点，那么当它发生故障时，集群将完全处于不可用状态。

不过 Redis 也提供了一个参数cluster-require-full-coverage可以允许部分节点故障，其它节点还可以继续提供对外访问。比如 7000 主节点宕机，作为 slave 的 7003 成为 Master 节点继续提供服务。当下线的节点 7000 重新上线，它将成为当前 70003 的从节点。

5.1 故障检测
一个节点认为某个节点失联了并不代表所有的节点都认为它失联了。*只有当大多数负责处理 slot 节点都认定了某个节点下线了，集群才认为该节点需要进行主从切换*。Redis 集群节点采用 Gossip协议来广播自己的状态以及自己对整个集群认知的改变。比如一个节点发现某个节点失联了 (*PFail*)，它会将这条信息向整个集群广播，其它节点也就可以收到这点失联信息。

如果一个节点收到了某个节点失联的数量 (PFail Count) 已经达到了集群的大多数，就可以标记该节点为确定下线状态 (*Fail*)，然后向整个集群广播，强迫其它节点也接收该节点已经下线的事实，并立即对该失联节点进行*主从切换*。

5.2 故障转移
当一个 Slave 发现自己的主节点进入已下线状态后，从节点将开始对下线的主节点进行故障转移。

从下线的 Master 及节点的 Slave 节点列表选择一个节点成为新主节点。
新主节点会撤销所有对已下线主节点的 slot 指派，并将这些 slots 指派给自己。
新的主节点向集群广播一条 *PONG 消息*，这条 PONG 消息可以让集群中的其他节点立即知道这个节点已经由从节点*变成了主节点*，并且这个主节点已经接管了原本由已下线节点负责处理的槽。
新的主节点开始接收处理槽有关的命令请求，故障转移完成。

5.3 选主流程
*集群的配置纪元 +1*，是一个自曾计数器，初始值 0 ，每次执行故障转移都会 +1。
检测到主节点下线的*从节点*向集群广播一条CLUSTERMSG_TYPE_FAILOVER_AUTH_REQUEST消息，要求所有收到这条消息、并且*具有投票权的主节点*向这个从节点投票。
这个主节点*尚未投票*给其他从节点，那么主节点将向要求投票的从节点返回一条CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK消息，表示这个主节点支持从节点成为新的主节点。
参与选举的从节点都会接收CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK消息，如果收集到的票 >= (N/2) + 1 支持，那么这个从节点就被选举为新主节点。
如果在一个配置纪元里面没有从节点能收集到足够多的支持票，那么集群进入一个*新的配置纪元*，并再次进行选举，直到选出新的主节点为止。
跟哨兵类似，两者都是基于 Raft 算法来实现的，流程如图所示：



六、redis集群扩容与缩容原理
随着应用场景的升级，缓存可能需要扩容，扩容的方式有两种：垂直扩容（Scale Up）和水平扩容（Scale Out)。垂直扩容无需详述。实际应用场景中，采用水平扩容更多一些，根据是否增加主节点数量，水平扩容方式有两种。

6.1 Redis集群的扩容方式

- 主节点数量不变

比如，当前有一台物理机 A，构建了一个包含3个 Redis 实例的集群；扩容时，我们新增一台物理机 B，拉起一个 Redis 实例并加入物理机 A 的集群；B 上 Redis 实例对 A 上的一个主节点进行复制，然后进行主备倒换；如此，Redis 集群还是3个主节点，只不过变成了 A2-B1 的结构，将一部分请求压力分担到了新增的节点上，同时物理容量上限也会增加，主要步骤如下：

将新增节点加入集群；
将新增节点设置为某个主节点的从节点，进而对其进行复制；
进行主备倒换，将新增的节点调整为主。

- 增加主节点数量

不增加主节点数量的方式扩容比较简单，但是，从负载均衡的角度来看，并不是很好的选择。例如，如果主节点数量较少，那么单个节点所负责的 Slot 的数量必然较多，很容易出现大量 Key 的读写集中于少数节点的现象，而增加主节点的数量，可以更有效的分摊访问压力，充分利用资源。主要步骤如下：

将新增节点加入集群；
将集群中的部分 *Slot 迁移*至新增的节点。

6.2 Redis集群的扩容原理
新节点刚开始都是master节点，但是由于没有负责的槽，所以不能接收任何读写操作，对新节点的后续操作，一般有两种选择：

- 从其他的节点迁移槽和数据给新节点
- 作为其他节点的slave负责故障转移

```
# 新节点加入集群
redis-trib.rb add-node new_host:new_port old_host:old_port
# 新节点加入集群并作为指定master的slave
redis-trib.rb add-node new_host:new_port old_host:old_port --slave --master-id <master-id>
```

建议使用redis-trib.rb add-node将新节点添加到集群中，该命令会检查新节点的状态，如果新节点已经加入了其他集群或者已经包含数据，则会报错，而使用cluster meet命令则不会做这样的检查，假如新节点已经存在数据，则会合并到集群中，造成数据不一致。

6.2.1 迁移slot和数据
假设原有3个master，每个master负责10384 / 3 ≈ 5461个slot
加入一个新的master之后，每个master负责10384 / 4 = 4096个slot
确定好迁移计划之后，例如，每个master将超过4096个slot的部分迁移到新的master中，然后开始以slot为单位进行迁移。


 slot迁移的其他说明

迁移过程是*同步的*，在目标节点执行*restore指令*到原节点删除key之间，原节点的主线程处于*阻塞状态*，直到key被删除成功
如果迁移过程突然出现网路故障，整个slot迁移只进行了一半，这时两个节点仍然会被标记为中间过滤状态，即"*migrating*"和"*importing*"，下次迁移工具连接上之后，会继续进行迁移
在迁移过程中，如果每个key的内容都很小，那么迁移过程很快，不会影响到客户端的正常访问
如果key的内容很大，由于迁移一个key的迁移过程是阻塞的，就会同时导致原节点和目标节点的卡顿，影响集群的稳定性，所以，集群环境下，业务逻辑要尽可能的避免大key的产生

6.3 Redis集群的缩容

如果下线的是slave，那么*通知*其他节点忘记下线的节点
如果下线的是master，那么将此master的slot*迁移*到其他master之后，*通知*其他节点忘记此master节点
其他节点都忘记了下线的节点之后，此节点就可以正常停止服务了

### 为什么RedisCluster无法使用pipeline?

管道（pipeline）在某些场景下非常有用，比如有*多个操作命令*需要被迅速提交至服务器端，但用户并*不依赖每个操作返回的响应结果*，*对结果响应也无需立即获得*，那么管道就可以用来作为优化性能的批处理工具。
性能提升的原因主要是*减少了 TCP 连接中交互往返的开销*。

不过在程序中使用管道请注意，使用 pipeline 时客户端将独占与服务器端的连接，此期间将不能进行其他“非管道”类型操作，直至 pipeline 被关闭；如果要同时执行其他操作，可以为 pipeline 操作单独建立一个连接，将其与常规操作分离开来。

从原理上来看，pipeline就是*用一个redis 的Socket连接 去多次执行redis命令（发送请求）而不必等待响应*，*当所有请求都执行完毕后再一次性的从这个socket中读取请求*。期间减少了在网络上的无用等待，通常会有3-10倍以上的速度提升.

具体的redis命令，会根据key计算出一个槽位（slot）,然后根据*槽位*去特定的节点redis上执行操作。

### 为什么哈希槽的数量是16384（2^14）个呢?

Redis 每个实例节点上都保存对应有哪些 slots，它✁一个 unsigned char slots[REDIS_CLUSTER_SLOTS/8]类型
在 redis 节点发送心跳包时需要把所有槽放到这个心跳包里，如果 slots 数量=65536 ，占空间= 65536 / 8(一个字节 8bit) / 1024(1024个字节 1kB) =8kB
如果使用 slots 数量✁ 16384 ，所占空间 = 16384 / 8(每个字节 8bit) / 1024(1024个字节 1kB) = 2kB ，可见 16384 个 slots 比 65536 省 6kB 内存左右，假如一个集群有 100 个节点,那每个实例里就省了 600kB 啦

一般情况下 Redis cluster 集群主节点数量基本不可能超过 1000 个，超过
1000会导致网络拥堵。对于节点数在 1000 以内✁ Redis cluster 集群，16384
个槽位其实够用了。

既然为了节省内存网络开销，为什么 slots 不选择用 8192（即 16384/2）呢?
Redis 将 key 换算成 slots ✁方法：其实就✁✁将 crc16(key) 之后再和 slots数量进行与计算
这里为什么用 0x3FFF(16383) 来计算,而不✁ 16384 呢？
因为在不产生溢出情况下 x % (2^n)等价于x & (2^n - 1)即 x % 16384 == x & 16383
